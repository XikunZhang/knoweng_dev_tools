{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KnowEnG-Research Clean and Run Samples Clustering\n",
    "\n",
    "#### Setup Required:\n",
    "* Clone Data Cleanup & Samples Clustering to the same directory as this notebook's parent directory.\n",
    "* Manually create a results directory in that same parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         load the library code\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, '../../Data_Cleanup_Pipeline/src')\n",
    "import data_cleanup_toolbox as dc\n",
    "\n",
    "sys.path.insert(1, '../../Samples_Clustering_Pipeline/src')\n",
    "import sample_clustering_toolbox as sc\n",
    "\n",
    "sys.path.insert(1, '../src')\n",
    "import KnowEnG_graphics as gu\n",
    "\n",
    "import knpackage\n",
    "import knpackage.toolbox as kn\n",
    "\n",
    "def view_dictionary(run_parameters):\n",
    "    for k in sorted(run_parameters.keys()):\n",
    "        print('run_parameters[',k,'] = \\n\\t',run_parameters[k],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload genomic  and/or  phenotypic spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_results                                   = '../../results'\n",
    "local_results                                   = kn.create_dir(os.path.abspath(local_results),'result') \n",
    "data_cleanup_yaml_dir                           = os.path.join(os.getcwd(), '../../Data_Cleanup_Pipeline/data/run_files')\n",
    "data_cleanup_pars                               = kn.get_run_parameters(data_cleanup_yaml_dir, 'TEMPLATE_data_cleanup.yml')\n",
    "data_cleanup_pars['pipeline_type']              = 'sample_clustering_pipeline'\n",
    "spreadsheet_path                                = '../../Samples_Clustering_Pipeline/data/spreadsheets'\n",
    "spreadsheet_file_name                           = 'tcga_ucec_somatic_mutation_data.df'\n",
    "data_cleanup_pars['spreadsheet_name_full_path'] =  os.path.join(spreadsheet_path, spreadsheet_file_name)\n",
    "pheno_file_name                                 = 'UCEC_phenotype.txt'\n",
    "data_cleanup_pars['phenotype_name_full_path']   = os.path.join(spreadsheet_path, pheno_file_name)\n",
    "data_cleanup_pars['run_directory']              = '../../Data_Cleanup_Pipeline/src'\n",
    "data_cleanup_pars['results_directory']          = local_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                           uncomment to view the parameters if necessary\n",
    "# view_dictionary(data_cleanup_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleanup run time  933.1685390472412\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df.\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt.\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No NA detected in row index.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Found 248 intersections between phenotype and spreadsheet data.\n",
      "INFO: Finished running sanity check for phenotype data.\n",
      "INFO: Start processing user spreadsheet data.\n",
      "INFO: Start to run sanity checks for user spreadsheet data.\n",
      "INFO: No NA detected in row index.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Mapped 17490 genes to ensemble name.\n",
      "INFO: Unable to map 57 genes to ensemble name.\n",
      "INFO: Finished running sanity check for user spreadsheet data.\n",
      "INFO: Cleaned user spreadsheet has 17490 row(s), 248 column(s).\n",
      "INFO: Cleaned phenotype data has 451 row(s), 19 column(s).\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df.\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt.\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No NA detected in row index.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Found 248 intersections between phenotype and spreadsheet data.\n",
      "INFO: Finished running sanity check for phenotype data.\n",
      "INFO: Start processing user spreadsheet data.\n",
      "INFO: Start to run sanity checks for user spreadsheet data.\n",
      "INFO: No NA detected in row index.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Mapped 17490 genes to ensemble name.\n",
      "INFO: Unable to map 57 genes to ensemble name.\n",
      "INFO: Finished running sanity check for user spreadsheet data.\n",
      "INFO: Cleaned user spreadsheet has 17490 row(s), 248 column(s).\n",
      "INFO: Cleaned phenotype data has 451 row(s), 19 column(s).\n",
      "\n",
      "Output Files:\n",
      "tcga_ucec_somatic_mutation_data_ETL.tsv\n",
      "tcga_ucec_somatic_mutation_data_MAP.tsv\n",
      "tcga_ucec_somatic_mutation_data_UNMAPPED.tsv\n",
      "UCEC_phenotype_ETL.tsv\n"
     ]
    }
   ],
   "source": [
    "tc0                    = time.time()\n",
    "STATUS, message_string = dc.run_samples_clustering_pipeline(data_cleanup_pars)\n",
    "cleanup_run_time       = time.time() - tc0\n",
    "\n",
    "print('Data Cleanup run time ', cleanup_run_time)\n",
    "\n",
    "for l in message_string:\n",
    "    print(l)\n",
    "if STATUS:\n",
    "    print('\\nOutput Files:')\n",
    "    output_files_list = os.listdir(data_cleanup_pars['results_directory'])\n",
    "    for file_name in output_files_list:\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the data using a samples_clustering_parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_clustering_yaml_dir                           = os.path.join(os.getcwd(), '../../Samples_Clustering_Pipeline/data/run_files')\n",
    "samples_clustering_pars                               = kn.get_run_parameters(samples_clustering_yaml_dir, 'BENCHMARK_7_SC_cc_net_nmf_parallel_shared.yml')\n",
    "spreadsheet_path                                      = data_cleanup_pars['results_directory']\n",
    "spreadsheet_file_name_base, ext                       = os.path.splitext(spreadsheet_file_name)\n",
    "spreadsheet_cleaned_file_name                         = spreadsheet_file_name_base + '_ETL.tsv'\n",
    "samples_clustering_pars['spreadsheet_name_full_path'] =  os.path.join(spreadsheet_path, spreadsheet_cleaned_file_name)\n",
    "phenotype_file_name_base, ext                         = os.path.splitext(pheno_file_name)\n",
    "phenotype_cleaned_file_name                           = phenotype_file_name_base + '_ETL.tsv'\n",
    "samples_clustering_pars['phenotype_name_full_path']   =  os.path.join(spreadsheet_path, phenotype_cleaned_file_name)\n",
    "network_path                                          = '../../Samples_Clustering_Pipeline/data/networks'\n",
    "network_file_name                                     = 'keg_ST90_4col.edge'\n",
    "samples_clustering_pars['gg_network_name_full_path']  =  os.path.join(network_path, network_file_name)\n",
    "samples_clustering_pars['run_directory']              = '../../Samples_Clustering_Pipeline/src'\n",
    "samples_clustering_pars['results_directory']          = data_cleanup_pars['results_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view_dictionary(samples_clustering_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and view the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_clock = time.time()\n",
    "\n",
    "sc.run_cc_net_nmf(samples_clustering_pars)\n",
    "\n",
    "run_time = time.time() - start_clock\n",
    "\n",
    "\n",
    "print('samples clustering run time:\\t', run_time, '\\n')\n",
    "results_dir_list           = os.listdir(samples_clustering_pars['results_directory'])\n",
    "cc_5mat                    = None\n",
    "cc_prefix                  = 'consensus_matrix'\n",
    "cluster_evaluation_prefix  = 'clustering_evaluation_result'\n",
    "cluster_eval_df = None\n",
    "for l in results_dir_list:\n",
    "    \n",
    "    if l[0:len(cc_prefix)]   == cc_prefix:\n",
    "        consensus_matrix_file = os.path.join(samples_clustering_pars['results_directory'], l)\n",
    "        consensus_df          = pd.read_csv(consensus_matrix_file, sep='\\t', header=0, index_col=0)\n",
    "        cc_5mat               = consensus_df.as_matrix()\n",
    "        \n",
    "    if l[0:len(cluster_evaluation_prefix)] == cluster_evaluation_prefix:\n",
    "        cluster_eval_filename               = os.path.join(samples_clustering_pars['results_directory'], l)\n",
    "        cluster_eval_df                     = pd.read_csv(cluster_eval_filename, sep='\\t', header=0, index_col=0)\n",
    "\n",
    "Maximum_Consensus_Matrix_Display_Width = 1200\n",
    "if cc_5mat is not None and cc_5mat.shape[1] < Maximum_Consensus_Matrix_Display_Width:\n",
    "    I0                                      = sc.form_consensus_matrix_graphic(cc_5mat, samples_clustering_pars[ 'number_of_clusters' ])\n",
    "    display(gu.mat_to_blue(I0))\n",
    "    \n",
    "if cluster_eval_df is not None:\n",
    "    display(cluster_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other data files available in\n",
      " /Users/lanier4/dlanier_KnowEnG/results/result1494010219371786 \n",
      "\n",
      "clustering_evaluation_result_Fri_05_May_2017_14_06_19.049815893.tsv\n",
      "consensus_matrix_cc_net_nmf_Fri_05_May_2017_14_06_18.042548894_viz.tsv\n",
      "genes_averages_by_cluster_cc_net_nmf_Fri_05_May_2017_14_06_28.620660066_viz.tsv\n",
      "genes_by_samples_heatmap_cc_net_nmf_Fri_05_May_2017_14_06_21.656224012_viz.tsv\n",
      "genes_variance_cc_net_nmf_Fri_05_May_2017_14_06_28.736527919_viz.tsv\n",
      "samples_label_by_cluster_cc_net_nmf_Fri_05_May_2017_14_06_18.172559976_viz.tsv\n",
      "silhouette_average_cc_net_nmf_Fri_05_May_2017_14_06_18.171514987_viz.tsv\n",
      "tcga_ucec_somatic_mutation_data_ETL.tsv\n",
      "tcga_ucec_somatic_mutation_data_MAP.tsv\n",
      "tcga_ucec_somatic_mutation_data_UNMAPPED.tsv\n",
      "top_genes_by_cluster_cc_net_nmf_Fri_05_May_2017_14_06_28.770649909_download.tsv\n",
      "UCEC_phenotype_ETL.tsv\n"
     ]
    }
   ],
   "source": [
    "print('Other data files available in\\n',samples_clustering_pars['results_directory'],'\\n')\n",
    "results_dir_list = os.listdir(samples_clustering_pars['results_directory'])\n",
    "for l in results_dir_list:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
