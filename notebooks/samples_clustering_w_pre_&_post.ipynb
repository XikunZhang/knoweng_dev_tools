{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo - mini pipeline - KnowEnG-Research\n",
    "\n",
    "#### Setup Required:\n",
    "* Create or select a suitable directory.\n",
    "* In that directory create the run and test directories: test/run_dir/results\n",
    "    * (these are referred to below and allow an easy transition to docker operation)\n",
    "* Clone all the repositories into that directory.\n",
    "    *  git clone https://github.com/dlanier/keg_test_tools.git\n",
    "    *  git clone https://github.com/KnowEnG-Research/Data_Cleanup_Pipeline.git\n",
    "    *  git clone https://github.com/KnowEnG-Research/Samples_Clustering_Pipeline.git\n",
    "* In the test directory for both KnonwEnG pipelines run \"make env_setup\"\n",
    "* Double check (painfully-slow) all the relative path names below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: the  KnowEnG-Research/KnowEnG_Pipelines_Library version is required here\n",
    "# with the KnowEnG-Research/Data_Cleanup_Pipeline.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '../../KnowEnG_Pipelines_Library')\n",
    "sys.path.insert(1, '../../KnowEnG_Pipelines_Library.knpackage')\n",
    "from knpackage import redis_utilities\n",
    "\n",
    "dcp_src = '../../Data_Cleanup_Pipeline/src/'\n",
    "sys.path.insert(1, dcp_src)\n",
    "import data_cleanup_toolbox as dc\n",
    "\n",
    "sys.path.insert(1, '../../keg_test_tools/src')\n",
    "import dcp_test\n",
    "\n",
    "sys.path.insert(1, '../../Samples_Clustering_Pipeline/src')\n",
    "import sample_clustering_toolbox as sc\n",
    "\n",
    "import knpackage.toolbox as kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_results = '../../test/run_dir'\n",
    "yaml_dir = '../../Data_Cleanup_Pipeline/data/run_files'\n",
    "yaml_file = 'TEMPLATE_data_cleanup.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   local convenience function\n",
    "def show_dictionary(a_dict):\n",
    "    for k in list(a_dict.keys()):\n",
    "        print(k,':\\t',a_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate and clean the data using a cleanup_parameters dictionary.\n",
    "* Use the spreadsheet_dir and spreadsheet_name variables in the next cell to locate your file.\n",
    "* The output will be in the local_results variable defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcga_ucec_somatic_mutation_data_ETL.tsv\n",
      "taxonid :\t 9606\n",
      "results_directory :\t ../../test/run_dir\n",
      "phenotype_full_path :\t ../data/spreadsheets/TEST_1_phenotype.tsv\n",
      "spreadsheet_name_full_path :\t ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df\n",
      "run_file :\t TEMPLATE_data_cleanup.yml\n",
      "source_hint :\t \n",
      "run_directory :\t ../../Data_Cleanup_Pipeline/src\n",
      "pipeline_type :\t sample_clustering_pipeline\n",
      "redis_credential :\t {'port': 6379, 'host': 'knowredis.knowhub.org', 'password': 'KnowEnG'}\n"
     ]
    }
   ],
   "source": [
    "# Get the template yaml file\n",
    "data_cleanup_pars = kn.get_run_parameters(yaml_dir, yaml_file)\n",
    "\n",
    "spreadsheet_dir = '../../Samples_Clustering_Pipeline/data/spreadsheets'\n",
    "spreadsheet_name = 'tcga_ucec_somatic_mutation_data.df'\n",
    "cleaned_spreadsheet_expected_name = spreadsheet_name[:-3] + '_ETL.tsv'\n",
    "\n",
    "data_cleanup_pars['spreadsheet_name_full_path'] = os.path.join(spreadsheet_dir,spreadsheet_name)\n",
    "\n",
    "data_cleanup_pars['run_directory'] = '../../Data_Cleanup_Pipeline/src'\n",
    "data_cleanup_pars['results_directory'] = local_results\n",
    "\n",
    "# gene_priorization_pipeline, sample_clustering_pipeline, geneset_characterization_pipeline\n",
    "data_cleanup_pars['pipeline_type'] = 'sample_clustering_pipeline'\n",
    "\n",
    "show_dictionary(data_cleanup_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#         Run the samples clustering pipeline to get the output files\n",
    "# SLOW SLOW --^<O>~<0>^-- unless running on the server with redis --^<O>~<0>^-- SLOW SLOW SLOW\n",
    "t_zero = time.time()\n",
    "STATUS, message_string = dc.run_samples_clustering_pipeline(data_cleanup_pars)\n",
    "for l in message_string:\n",
    "    print(l)\n",
    "cleanup_run_time = time.time() - t_zero\n",
    "if STATUS:\n",
    "    print('data_cleanup_toolbox.run_samples_clustering_pipeline SUCCEEDED IN %0.3f sec'%(\n",
    "        cleanup_run_time))\n",
    "    output_files_list = os.listdir(data_cleanup_pars['results_directory'])\n",
    "    print('\\n')\n",
    "    for file_name in output_files_list:\n",
    "        print(file_name)\n",
    "else:\n",
    "    print('data_cleanup_toolbox.run_samples_clustering_pipeline SUCCEEDED IN %0.3f sec'%(\n",
    "        cleanup_run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the data using a samples_clustering_parameters dictionary\n",
    "\n",
    "### Note that you may need to run \"make env_setup\" from the .../Samples_Clustering_Pipeline/test    directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_sampling_fraction :\t 0.8\n",
      "rwr_restart_probability :\t 0.5\n",
      "run_directory :\t ../../Samples_Clustering_Pipeline/src\n",
      "parallelism :\t 4\n",
      "rwr_max_iterations :\t 100\n",
      "results_directory :\t /Users/mojo/BigDataTank/trifecta_tank\n",
      "run_file :\t BENCHMARK_7_SC_cc_net_nmf_parallel_shared.yml\n",
      "tmp_directory :\t /Users/mojo/BigDataTank/trifecta_tank\n",
      "top_number_of_genes :\t 100\n",
      "nmf_penalty_parameter :\t 1400\n",
      "nmf_max_iterations :\t 10000\n",
      "processing_method :\t parallel\n",
      "rwr_convergence_tolerence :\t 0.0001\n",
      "gg_network_name_full_path :\t ../../Samples_Clustering_Pipeline/data/networks/keg_ST90_4col.edge\n",
      "nmf_conv_check_freq :\t 50\n",
      "nmf_max_invariance :\t 200\n",
      "spreadsheet_name_full_path :\t /Users/mojo/BigDataTank/trifecta_tank/tcga_ucec_somatic_mutation_data_ETL.tsv\n",
      "number_of_bootstraps :\t 20\n",
      "rows_sampling_fraction :\t 0.8\n",
      "method :\t cc_net_nmf\n",
      "number_of_clusters :\t 3\n"
     ]
    }
   ],
   "source": [
    "samples_clustering_yaml_dir = os.path.join(os.getcwd(), \n",
    "                                           '../../Samples_Clustering_Pipeline/data/run_files')\n",
    "samples_clustering_pars = kn.get_run_parameters(samples_clustering_yaml_dir, \n",
    "                                            'BENCHMARK_7_SC_cc_net_nmf_parallel_shared.yml')\n",
    "\n",
    "spreadsheet_dir = data_cleanup_pars['results_directory']\n",
    "spreadsheet_name = cleaned_spreadsheet_expected_name\n",
    "samples_clustering_pars['spreadsheet_name_full_path'] = os.path.join(\n",
    "    spreadsheet_dir, spreadsheet_name)\n",
    "network_path = '../../Samples_Clustering_Pipeline/data/networks'\n",
    "network_file_name = 'keg_ST90_4col.edge'\n",
    "samples_clustering_pars['gg_network_name_full_path'] =\\\n",
    "    os.path.join(network_path, network_file_name)\n",
    "\n",
    "samples_clustering_pars.pop('phenotype_name_full_path', None)\n",
    "\n",
    "samples_clustering_pars['run_directory'] = '../../Samples_Clustering_Pipeline/src'\n",
    "samples_clustering_pars['results_directory'] = local_results\n",
    "samples_clustering_pars['tmp_directory'] = '/Users/mojo/BigDataTank/trifecta_tank'\n",
    "\n",
    "samples_clustering_pars['number_of_bootstraps'] = 20\n",
    "samples_clustering_pars['number_of_clusters'] = 3\n",
    "\n",
    "samples_clustering_pars['rows_sampling_fraction'] = 0.8\n",
    "samples_clustering_pars['cols_sampling_fraction'] = 0.8\n",
    "\n",
    "samples_clustering_pars['rwr_restart_probability'] = 0.5\n",
    "\n",
    "show_dictionary(samples_clustering_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t run_cc_net_nmf\n",
      "spreadsheet_mat (11128, 248): 112\n",
      "network_mat (11128, 11128): 56\n",
      "lap_diag (11128, 11128)\n",
      "lap_pos (11128, 11128)\n",
      "saved spreadsheet_mat to /Users/mojo/keg_tmp/keg_notebooks/A_Top_Secret/test_file_4_npdump in 0.0869 seconds\n",
      "In find_and_save_cc_net_nmf_clusters_parallel: file name is  /Users/mojo/keg_tmp/keg_notebooks/A_Top_Secret/test_file_4_npdump\n",
      "spreadsheet_mat = \n",
      "1.0\n",
      "Using number_of_cores = 20\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "spreadsheet_mat shape =  (11128, 248)\n",
      "\n",
      "\t\tSun Mar 12 22:15:10 2017 \n",
      "\t\tTemporary files written: getting consensus matrix\n",
      "\n",
      "\n",
      "\t\tConsensus Matrix Complete: 0.022 calling k-means\n",
      "\n",
      "\t\tk-means completed in 0.047 seconds\n",
      "\n",
      "samples clustering run time:\t 119.68846893310547\n"
     ]
    }
   ],
   "source": [
    "# Note that the printed output below is from a development version\n",
    "# of samples clustering dependencies.\n",
    "start_clock = time.time()\n",
    "sc.run_cc_net_nmf(samples_clustering_pars)\n",
    "run_time = time.time() - start_clock\n",
    "print('samples clustering run time:\\t', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consensus_matrix_cc_net_nmf_Sun_12_Mar_2017_17_15_10.206341028_viz.tsv\n",
      "genes_averages_by_cluster_cc_net_nmf_Sun_12_Mar_2017_17_15_21.663466930_viz.tsv\n",
      "genes_by_samples_heatmap_cc_net_nmf_Sun_12_Mar_2017_17_15_12.265441894_viz.tsv\n",
      "genes_variance_cc_net_nmf_Sun_12_Mar_2017_17_15_21.768404960_viz.tsv\n",
      "samples_label_by_cluster_cc_net_nmf_Sun_12_Mar_2017_17_15_10.395998954_viz.tsv\n",
      "silhouette_average_cc_net_nmf_Sun_12_Mar_2017_17_15_10.393580913_viz.tsv\n",
      "tcga_ucec_somatic_mutation_data_ETL.tsv\n",
      "tcga_ucec_somatic_mutation_data_MAP.tsv\n",
      "tcga_ucec_somatic_mutation_data_UNMAPPED.tsv\n",
      "top_genes_by_cluster_cc_net_nmf_Sun_12_Mar_2017_17_15_21.816582918_download.tsv\n"
     ]
    }
   ],
   "source": [
    "results_dir_list = os.listdir(samples_clustering_pars['results_directory'])\n",
    "for l in results_dir_list:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
