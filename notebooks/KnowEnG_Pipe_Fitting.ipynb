{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KnowEnG Pipelines Notebook\n",
    "\n",
    "## Setup the Knowledge Engene for Genomics Pipelines.\n",
    "\n",
    "* Output from one or more pipelines may be used as input to another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1! Create a directory and follow the instructions in the repository to setup your local computer.\n",
    "* Move this notebook to the directory where the pipelines have been cloned from git hub.\n",
    "\n",
    "### 2! Clone the code from github.\n",
    "* In your directory, cut and paste to the command line:\n",
    "*  git clone https://github.com/KnowEnG-Research/KnowEnG_Pipelines_Library.git\n",
    "*  git clone https://github.com/KnowEnG-Research/Data_Cleanup_Pipeline.git\n",
    "*  git clone https://github.com/KnowEnG-Research/Clustering_Evaluation.git\n",
    "*  git clone https://github.com/KnowEnG-Research/Gene_Prioritization_Pipeline.git\n",
    "*  git clone https://github.com/KnowEnG-Research/GeneSet_Characterization_Pipeline.git\n",
    "*  git clone https://github.com/KnowEnG-Research/Samples_Clustering_Pipeline.git\n",
    "\n",
    "### 3! Run the next cell to import the libraries for this notebook page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, '../clones/KnowEnG_Pipelines_Library')\n",
    "sys.path.insert(1, '../clones/KnowEnG_Pipelines_Library/knpackage')\n",
    "import redis_utilities as redisutil\n",
    "\n",
    "sys.path.insert(1, '../clones/Data_Cleanup_Pipeline/src')\n",
    "import data_cleanup_toolbox as dc_tbx\n",
    "\n",
    "sys.path.insert(1, '../clones/Clustering_Evaluation/src')\n",
    "import clustering_eval_toolbox as ce_tbx\n",
    "\n",
    "sys.path.insert(1, '../clones/Gene_Prioritization_Pipeline/src')\n",
    "import gene_prioritization_toolbox as gp_tbx\n",
    "\n",
    "sys.path.insert(1, '../clones/GeneSet_Characterization_Pipeline/src')\n",
    "import geneset_characterization_toolbox as gsc_tbx\n",
    "\n",
    "sys.path.insert(1, '../clones/Samples_Clustering_Pipeline/src')\n",
    "import sample_clustering_toolbox as sc_tbx\n",
    "\n",
    "import knpackage.toolbox as kn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4! Type in: local data and results directory names into the quoted lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_dir = '/Users/mojo/'\n",
    "\n",
    "testy_data_dir = os.path.join(local_dir, 'BigDataTank/pipeline_spreadsheets/raw')\n",
    "testy_data_file_list = os.listdir(testy_data_dir)\n",
    "\n",
    "# The next lines create a data directory.\n",
    "this_dir = '../clones'\n",
    "results_dir = kn.create_dir(this_dir, 'keggo_leggo_test')\n",
    "\n",
    "# These lines get the path names for installation testing the cloned repositories.\n",
    "ce_dir = os.path.join(this_dir, 'Clustering_Evaluation/data')\n",
    "dc_dir = os.path.join(this_dir, 'Data_Cleanup_Pipeline/data')\n",
    "\n",
    "gp_dir = os.path.join(this_dir, 'Gene_Prioritization_Toolbox/data')\n",
    "gc_dir = os.path.join(this_dir, 'GeneSet_Characterization_Toolbox/data')\n",
    "sc_dir = os.path.join(this_dir, 'Samples_Clustering_Pipeline/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Samples Clustering: enter the name of the spreadsheet, network and phenotype data files.\n",
    "* The cloned samples clustering directory structure below has a network you may use\n",
    "* Note that there are three changes of run parameter dictionaries in the samples clustering pipeline.\n",
    "\n",
    "   * 1) the cleanup parameters that describe the source files\n",
    "   * 2) the Samples Clustering run time options parameters\n",
    "   * 3) the clustering evaluation post-processing parameters\n",
    "   \n",
    "\n",
    "* Each pipeline has \"yaml\" files for a python dictionary in its associated ...Pipeline/data/run_files/ directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5! Setup a python \"dictionary\" for choosing and cleaning the input data:\n",
    "* Then run the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yml_cleanup_file = 'run_files/data_cleanup.yml'\n",
    "cleanup_parameters = kn.get_run_parameters(run_directory=dc_dir, run_file=yml_cleanup_file)\n",
    "\n",
    "cleanup_parameters['results_directory'] = results_dir\n",
    "cleanup_parameters['spreadsheet_name_full_path'] = os.path.join(testy_data_dir,'Hsap.ccle.G.gene_mut.binary.df')\n",
    "cleanup_parameters['phenotype_full_path'] = os.path.join(testy_data_dir,'Hsap.ccle.P.cell_line_info.mixed.df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6! Next step requires internet connection - clean the data: check and translate the gene names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_flag =  True\n",
      "message =  This is a valid user spreadsheet. Proceed to next step analysis.\n"
     ]
    }
   ],
   "source": [
    "validation_flag, message = dcp_dc_tbx.run_samples_clustering_pipeline(cleanup_parameters)\n",
    "print('validation_flag = ', validation_flag)\n",
    "print('message = ', message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The message above shoule look something like:\n",
    "* validation_flag =  True\n",
    "* message =  This is a valid user spreadsheet. Proceed to next step analysis.\n",
    "\n",
    "#### If so setup and run the next cell by getting the file names from the results directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data_dir = results_dir\n",
    "phenotypes_file = os.path.join(clean_data_dir, 'Hsap.ccle.P.cell_line_info.mixed_ETL.tsv')\n",
    "\n",
    "run_parameters = kn.get_run_parameters(sc_dir, 'run_files/BENCHMARK_7_SC_cc_net_nmf_parallel_shared.yml')\n",
    "run_parameters['spreadsheet_name_full_path'] = os.path.join(clean_data_dir, 'Hsap.ccle.G.gene_mut.binary_ETL.tsv')\n",
    "\n",
    "run_parameters.pop('phenotype_name_full_path', None)\n",
    "run_parameters['number_of_bootstraps'] = 10\n",
    "\n",
    "run_parameters['gg_network_name_full_path'] = os.path.join(sc_dir, 'networks/keg_ST90_4col.edge')\n",
    "run_parameters['results_directory'] = results_dir\n",
    "\n",
    "run_parameters['processing_method'] = parallel   # available methods: serial, parallel, distribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell does a lot of processing and may take a long time - ipython notebooks are slow compared to running on bare metal.\n",
    "* The yaml file in the above cell is set to parallel by default.\n",
    "* This may cause the notebook to stop responding and not write the files to the results directory.\n",
    "* Try setting the method to serial.\n",
    "* The 'number_of_bootstraps' key will increase the running time in linear proprotion.\n",
    "* The 'number_of_clusters' key will slow things down in exponential proportion - 7 or 8 might be too much for a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Clustering finished in 151.167 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "sc_tbx.run_cc_net_nmf(run_parameters)\n",
    "print('Samples Clustering finished in %0.3f seconds'%(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output file names are time-stamped so the next dictionary requires more manual attention before the second cell below.\n",
    "* identify the file you need by the prefix 'samples_label_by_cluster_cc_net_nmf_ ...  _viz.tsv' and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ce_parameters = kn.get_run_parameters(ce_dir, 'run_files/BENCHMARK_1_cluster_eval.yml')\n",
    "ce_parameters['cluster_mapping_full_path'] =\\\n",
    "    os.path.join(results_dir,'samples_label_by_cluster_cc_net_nmf_Wed_15_Feb_2017_17_04_27.845877885_viz.tsv')\n",
    "ce_parameters['phenotype_data_full_path'] = os.path.join(clean_data_dir, 'Hsap.ccle.P.cell_line_info.mixed_ETL.tsv')\n",
    "ce_parameters['results_directory'] = results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ce_tbx.clustering_evaluation(ce_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The results are all in the 'results_directory' and may be viewed in another notebook\n",
    "#### The ouput files (in reverse order) are listed below\n",
    "* clustering_evaluation_result_ ... .tsv  - stastical view of the clustering vs the phenotype data.\n",
    "* silhouette_average_cc_net_nmf_ ... _viz.tsv  - statistic says something about the chosen 'number_of_clusters'.\n",
    "* top_genes_by_cluster_cc_net_nmf_ ... _download.tsv  - very sparse view of which genes are most associated with clustering.\n",
    "* genes_averages_by_cluster_cc_net_nmf_ ... _viz.tsv  - how strongly each gene is in each cluster.\n",
    "* genes_variance_cc_net_nmf_ ... _viz.tsv  - the variance of each gene.\n",
    "* genes_by_samples_heatmap_cc_net_nmf_ ... _viz.tsv  - the (network smoothed) spreadsheet that was clusterd.\n",
    "* consensus_matrix_cc_net_nmf_ .. _viz.tsv  - the samples x samples consensus matrix (if consensus clustering was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
